{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543e809a-7122-4701-b7ef-63655b152a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /climatewatch/raw\n",
      "Created directory: /climatewatch/processed\n",
      "Created directory: /climatewatch/models\n",
      "Created directory: /climatewatch/temp\n",
      "Loaded 10 rows from CSV\n",
      "Written data to HDFS: /climatewatch/raw/articles.parquet\n",
      "Read 10 rows from HDFS\n",
      "\n",
      "Contents of /climatewatch:\n",
      "- models\n",
      "- processed\n",
      "- raw\n",
      "- temp\n"
     ]
    }
   ],
   "source": [
    "# İlk hücre\n",
    "import os\n",
    "import io\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# İkinci hücre\n",
    "# HDFS bağlantısını yapılandır\n",
    "hdfs_client = InsecureClient('http://hdfs-namenode:9870', user='root')\n",
    "\n",
    "# HDFS'de dizin oluştur\n",
    "base_path = '/climatewatch'\n",
    "subdirs = ['raw', 'processed', 'models', 'temp']\n",
    "\n",
    "for subdir in subdirs:\n",
    "    path = f'{base_path}/{subdir}'\n",
    "    try:\n",
    "        hdfs_client.makedirs(path)\n",
    "        print(f'Created directory: {path}')\n",
    "    except Exception as e:\n",
    "        print(f'Directory already exists or error: {path} - {str(e)}')\n",
    "\n",
    "# Üçüncü hücre\n",
    "# Örnek veri yükleme\n",
    "try:\n",
    "    df = pd.read_csv('/home/jovyan/work/scraped_articles_with_sentiment.csv')\n",
    "    print(f'Loaded {len(df)} rows from CSV')\n",
    "except Exception as e:\n",
    "    print(f'Error loading CSV: {str(e)}')\n",
    "    # Örnek veri oluştur\n",
    "    df = pd.DataFrame({\n",
    "        'title': ['Sample Article 1', 'Sample Article 2'],\n",
    "        'content': ['Climate change effects', 'Environmental impact'],\n",
    "        'sentiment': [0.5, -0.2]\n",
    "    })\n",
    "\n",
    "# Dördüncü hücre\n",
    "# DataFrame'i Parquet formatında HDFS'e yaz\n",
    "hdfs_path = f'{base_path}/raw/articles.parquet'\n",
    "\n",
    "# DataFrame'i Parquet formatına dönüştür\n",
    "table = pa.Table.from_pandas(df)\n",
    "buf = io.BytesIO()\n",
    "pq.write_table(table, buf)\n",
    "buf.seek(0)\n",
    "\n",
    "# HDFS'e yaz\n",
    "with hdfs_client.write(hdfs_path, overwrite=True) as writer:\n",
    "    writer.write(buf.getvalue())\n",
    "\n",
    "print(f'Written data to HDFS: {hdfs_path}')\n",
    "\n",
    "# Beşinci hücre\n",
    "# HDFS'den veriyi oku\n",
    "with hdfs_client.read(hdfs_path) as reader:\n",
    "    buf = io.BytesIO(reader.read())\n",
    "    buf.seek(0)\n",
    "    table = pq.read_table(buf)\n",
    "    df_from_hdfs = table.to_pandas()\n",
    "\n",
    "print(f'Read {len(df_from_hdfs)} rows from HDFS')\n",
    "df_from_hdfs.head()\n",
    "\n",
    "# Altıncı hücre\n",
    "# HDFS dizin içeriğini listele\n",
    "contents = hdfs_client.list(base_path)\n",
    "print(f'\\nContents of {base_path}:')\n",
    "for item in contents:\n",
    "    print(f'- {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cadb4-effe-430b-8d55-0a2d2ea437f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ed3ff-ad8a-4aca-8406-df88a6cb21f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
