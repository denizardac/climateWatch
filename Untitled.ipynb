{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b5b082-329b-4143-bd85-21aff8a8b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDELT verilerini okuyorum...\n",
      "GDELT veri sayısı: 174635\n",
      "\n",
      "İklim verilerini okuyorum...\n",
      "İklim veri sayısı: 319\n",
      "\n",
      "Verileri dönüştürüyorum...\n",
      "\n",
      "Verileri birleştiriyorum...\n",
      "\n",
      "Sonuçları kaydediyorum...\n",
      "Birleşik veri /home/jovyan/work/data_storage/processed/combined_data.parquet olarak kaydedildi.\n",
      "\n",
      "Birleştirilmiş veriden örnek:\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "|date|     _c1|   _c2| _c3|      _c4| _c5|     _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|    _c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|             _c34|_c35|                _c36|_c37|_c38|   _c39|   _c40|    _c41|_c42|                _c43|_c44|_c45|     _c46|    _c47|    _c48|_c49|                _c50|_c51|_c52|     _c53|    _c54|    _c55|    _c56|                _c57|      _c58|news_source_type|title|text|Source|year|temperature_anomaly|climate_source_type|\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "|NULL|       1|     2|   3|        4|   5|       6|   7|   8|   9|  10|  11|  12|  13|  14|  15|      16|  17|  18|  19|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  30|  31|  32|  33|               34|  35|                  36|  37|  38|     39|     40|      41|  42|                  43|  44|  45|       46|      47|      48|  49|                  50|  51|  52|       53|      54|      55|      56|                  57|      date|            news|   27|  28|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| BUS|INVESTOR|NULL|NULL|NULL|NULL|NULL| BUS|NULL|NULL|   0|  60|  60|   6|   2| 6.0|  10|   1|  10|-1.02974828375286|   0|                NULL|NULL|NULL|   NULL|   NULL|    NULL|   1|       United States|  US|  US|39.828175|-98.5795|      US|   1|       United States|  US|  US|39.828175|-98.5795|      US|20240101|https://www.thebl...|2024-01-01|            news|   60|   6|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| CHN|   CHINA| CHN|NULL|NULL|NULL|NULL|NULL|NULL|NULL| TWN|  TAIWAN| TWN|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  50|  50|   5|   1| 3.5|   2|   1|   2|             -0.8|   4|Beijing, Beijing,...|  CH|CH22|39.9289|116.388|-1898541|   4|Beijing, Beijing,...|  CH|CH22|  39.9289| 116.388|-1898541|   4|Beijing, Beijing,...|  CH|CH22|  39.9289| 116.388|-1898541|20240101|https://news.yaho...|2024-01-01|            news|   50|   5|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| FRA|  FRANCE| FRA|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  51|  51|   5|   1| 3.4|   2|   1|   2|              0.0|   4|Ikebukuro, Tokyo,...|  JA|JA40|35.7302|139.711| -230239|   0|                NULL|NULL|NULL|     NULL|    NULL|    NULL|   1|              France|  FR|  FR|     46.0|     2.0|      FR|20240101|https://nichegame...|2024-01-01|            news|   51|   5|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| JPN|MIYAZAKI| JPN|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  51|  51|   5|   1| 3.4|   6|   1|   6|              0.0|   4|Ikebukuro, Tokyo,...|  JA|JA40|35.7302|139.711| -230239|   0|                NULL|NULL|NULL|     NULL|    NULL|    NULL|   4|Ikebukuro, Tokyo,...|  JA|JA40|  35.7302| 139.711| -230239|20240101|https://nichegame...|2024-01-01|            news|   51|   5|  NULL|NULL|               NULL|               NULL|\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, lower, lit\n",
    "\n",
    "# Spark session başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClimateWatch ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Veri yolları\n",
    "GDELT_PATH = \"/home/jovyan/work/data_storage/gdelt/*.csv\"\n",
    "CLIMATE_PATH = \"/home/jovyan/work/data_storage/climate/*.csv\"\n",
    "OUTPUT_PATH = \"/home/jovyan/work/data_storage/processed/combined_data.parquet\"\n",
    "\n",
    "# 1. Veri Okuma\n",
    "print(\"GDELT verilerini okuyorum...\")\n",
    "gdelt_df = spark.read.option(\"header\", False).csv(GDELT_PATH)\n",
    "print(f\"GDELT veri sayısı: {gdelt_df.count()}\")\n",
    "\n",
    "print(\"\\nİklim verilerini okuyorum...\")\n",
    "climate_df = spark.read.option(\"header\", True).csv(CLIMATE_PATH)\n",
    "print(f\"İklim veri sayısı: {climate_df.count()}\")\n",
    "\n",
    "# 2. Dönüşüm ve Temizlik\n",
    "print(\"\\nVerileri dönüştürüyorum...\")\n",
    "gdelt_df = gdelt_df \\\n",
    "    .withColumnRenamed(\"_c0\", \"date\") \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"news_source_type\", lit(\"news\")) \\\n",
    "    .withColumn(\"title\", lower(col(\"_c27\"))) \\\n",
    "    .withColumn(\"text\", lower(col(\"_c28\")))\n",
    "\n",
    "# Climate verisi için dönüşüm\n",
    "climate_df = climate_df \\\n",
    "    .withColumnRenamed(\"Year\", \"year\") \\\n",
    "    .withColumnRenamed(\"Mean\", \"temperature_anomaly\") \\\n",
    "    .withColumn(\"climate_source_type\", lit(\"climate\"))\n",
    "\n",
    "# 3. Birleştirme\n",
    "print(\"\\nVerileri birleştiriyorum...\")\n",
    "climate_gdelt_joined = gdelt_df.join(\n",
    "    climate_df,\n",
    "    gdelt_df.date.substr(1, 4) == climate_df.year.cast(\"string\"),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Sonuçları Kaydet\n",
    "print(\"\\nSonuçları kaydediyorum...\")\n",
    "climate_gdelt_joined.write.mode(\"overwrite\").parquet(OUTPUT_PATH)\n",
    "print(f\"Birleşik veri {OUTPUT_PATH} olarak kaydedildi.\")\n",
    "\n",
    "# 5. Örnek verileri göster\n",
    "print(\"\\nBirleştirilmiş veriden örnek:\")\n",
    "climate_gdelt_joined.show(5)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9a419-196d-44f2-a5b3-08c5da84fe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a291ac7b-74b6-4763-9437-af62c8e3d246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB bağlantısını test ediyorum...\n",
      "MongoDB'ye bağlantı başarılı!\n",
      "Mevcut veritabanları: ['admin', 'config', 'local']\n",
      "\n",
      "GDELT verilerini okuyorum...\n",
      "GDELT veri sayısı: 174635\n",
      "\n",
      "İklim verilerini okuyorum...\n",
      "İklim veri sayısı: 319\n",
      "\n",
      "Verileri dönüştürüyorum...\n",
      "\n",
      "Verileri birleştiriyorum...\n",
      "\n",
      "Sonuçları kaydediyorum...\n",
      "Birleşik veri /home/jovyan/work/data_storage/processed/combined_data.parquet olarak kaydedildi.\n",
      "\n",
      "MongoDB'ye kaydediyorum...\n",
      "GDELT verileri MongoDB'ye kaydedildi: 174635 kayıt\n",
      "İklim verileri MongoDB'ye kaydedildi: 319 kayıt\n",
      "\n",
      "Birleştirilmiş veriden örnek:\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "|date|     _c1|   _c2| _c3|      _c4| _c5|     _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|    _c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|             _c34|_c35|                _c36|_c37|_c38|   _c39|   _c40|    _c41|_c42|                _c43|_c44|_c45|     _c46|    _c47|    _c48|_c49|                _c50|_c51|_c52|     _c53|    _c54|    _c55|    _c56|                _c57|      _c58|news_source_type|title|text|Source|year|temperature_anomaly|climate_source_type|\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "|NULL|       1|     2|   3|        4|   5|       6|   7|   8|   9|  10|  11|  12|  13|  14|  15|      16|  17|  18|  19|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  30|  31|  32|  33|               34|  35|                  36|  37|  38|     39|     40|      41|  42|                  43|  44|  45|       46|      47|      48|  49|                  50|  51|  52|       53|      54|      55|      56|                  57|      date|            news|   27|  28|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| BUS|INVESTOR|NULL|NULL|NULL|NULL|NULL| BUS|NULL|NULL|   0|  60|  60|   6|   2| 6.0|  10|   1|  10|-1.02974828375286|   0|                NULL|NULL|NULL|   NULL|   NULL|    NULL|   1|       United States|  US|  US|39.828175|-98.5795|      US|   1|       United States|  US|  US|39.828175|-98.5795|      US|20240101|https://www.thebl...|2024-01-01|            news|   60|   6|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| CHN|   CHINA| CHN|NULL|NULL|NULL|NULL|NULL|NULL|NULL| TWN|  TAIWAN| TWN|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  50|  50|   5|   1| 3.5|   2|   1|   2|             -0.8|   4|Beijing, Beijing,...|  CH|CH22|39.9289|116.388|-1898541|   4|Beijing, Beijing,...|  CH|CH22|  39.9289| 116.388|-1898541|   4|Beijing, Beijing,...|  CH|CH22|  39.9289| 116.388|-1898541|20240101|https://news.yaho...|2024-01-01|            news|   50|   5|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| FRA|  FRANCE| FRA|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  51|  51|   5|   1| 3.4|   2|   1|   2|              0.0|   4|Ikebukuro, Tokyo,...|  JA|JA40|35.7302|139.711| -230239|   0|                NULL|NULL|NULL|     NULL|    NULL|    NULL|   1|              France|  FR|  FR|     46.0|     2.0|      FR|20240101|https://nichegame...|2024-01-01|            news|   51|   5|  NULL|NULL|               NULL|               NULL|\n",
      "|NULL|20230101|202301|2023|2023.0027| JPN|MIYAZAKI| JPN|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|    NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|   1|  51|  51|   5|   1| 3.4|   6|   1|   6|              0.0|   4|Ikebukuro, Tokyo,...|  JA|JA40|35.7302|139.711| -230239|   0|                NULL|NULL|NULL|     NULL|    NULL|    NULL|   4|Ikebukuro, Tokyo,...|  JA|JA40|  35.7302| 139.711| -230239|20240101|https://nichegame...|2024-01-01|            news|   51|   5|  NULL|NULL|               NULL|               NULL|\n",
      "+----+--------+------+----+---------+----+--------+----+----+----+----+----+----+----+----+----+--------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----------------+----+--------------------+----+----+-------+-------+--------+----+--------------------+----+----+---------+--------+--------+----+--------------------+----+----+---------+--------+--------+--------+--------------------+----------+----------------+-----+----+------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, lower, lit\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# MongoDB bağlantısını test et\n",
    "print(\"MongoDB bağlantısını test ediyorum...\")\n",
    "client = MongoClient('mongodb://mongodb:27017/')\n",
    "db = client['climatewatch']\n",
    "print(\"MongoDB'ye bağlantı başarılı!\")\n",
    "print(\"Mevcut veritabanları:\", client.list_database_names())\n",
    "\n",
    "# Spark session başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClimateWatch ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Veri yolları\n",
    "GDELT_PATH = \"/home/jovyan/work/data_storage/gdelt/*.csv\"\n",
    "CLIMATE_PATH = \"/home/jovyan/work/data_storage/climate/*.csv\"\n",
    "OUTPUT_PATH = \"/home/jovyan/work/data_storage/processed/combined_data.parquet\"\n",
    "\n",
    "# 1. Veri Okuma\n",
    "print(\"\\nGDELT verilerini okuyorum...\")\n",
    "gdelt_df = spark.read.option(\"header\", False).csv(GDELT_PATH)\n",
    "print(f\"GDELT veri sayısı: {gdelt_df.count()}\")\n",
    "\n",
    "print(\"\\nİklim verilerini okuyorum...\")\n",
    "climate_df = spark.read.option(\"header\", True).csv(CLIMATE_PATH)\n",
    "print(f\"İklim veri sayısı: {climate_df.count()}\")\n",
    "\n",
    "# 2. Dönüşüm ve Temizlik\n",
    "print(\"\\nVerileri dönüştürüyorum...\")\n",
    "gdelt_df = gdelt_df \\\n",
    "    .withColumnRenamed(\"_c0\", \"date\") \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"news_source_type\", lit(\"news\")) \\\n",
    "    .withColumn(\"title\", lower(col(\"_c27\"))) \\\n",
    "    .withColumn(\"text\", lower(col(\"_c28\")))\n",
    "\n",
    "# Climate verisi için dönüşüm\n",
    "climate_df = climate_df \\\n",
    "    .withColumnRenamed(\"Year\", \"year\") \\\n",
    "    .withColumnRenamed(\"Mean\", \"temperature_anomaly\") \\\n",
    "    .withColumn(\"climate_source_type\", lit(\"climate\"))\n",
    "\n",
    "# 3. Birleştirme\n",
    "print(\"\\nVerileri birleştiriyorum...\")\n",
    "climate_gdelt_joined = gdelt_df.join(\n",
    "    climate_df,\n",
    "    gdelt_df.date.substr(1, 4) == climate_df.year.cast(\"string\"),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Sonuçları Kaydet\n",
    "print(\"\\nSonuçları kaydediyorum...\")\n",
    "climate_gdelt_joined.write.mode(\"overwrite\").parquet(OUTPUT_PATH)\n",
    "print(f\"Birleşik veri {OUTPUT_PATH} olarak kaydedildi.\")\n",
    "\n",
    "# 5. MongoDB'ye kaydet\n",
    "print(\"\\nMongoDB'ye kaydediyorum...\")\n",
    "# GDELT verilerini MongoDB'ye kaydet\n",
    "gdelt_collection = db['gdelt_events']\n",
    "gdelt_collection.delete_many({})  # Mevcut verileri temizle\n",
    "gdelt_pandas = gdelt_df.toPandas()\n",
    "gdelt_records = gdelt_pandas.to_dict('records')\n",
    "gdelt_collection.insert_many(gdelt_records)\n",
    "print(f\"GDELT verileri MongoDB'ye kaydedildi: {len(gdelt_records)} kayıt\")\n",
    "\n",
    "# İklim verilerini MongoDB'ye kaydet\n",
    "climate_collection = db['climate_data']\n",
    "climate_collection.delete_many({})  # Mevcut verileri temizle\n",
    "climate_pandas = climate_df.toPandas()\n",
    "climate_records = climate_pandas.to_dict('records')\n",
    "climate_collection.insert_many(climate_records)\n",
    "print(f\"İklim verileri MongoDB'ye kaydedildi: {len(climate_records)} kayıt\")\n",
    "\n",
    "# 6. Örnek verileri göster\n",
    "print(\"\\nBirleştirilmiş veriden örnek:\")\n",
    "climate_gdelt_joined.show(5)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f3d735-2b42-4f60-b277-fb833bfc1d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDELT verilerini okuyorum...\n",
      "GDELT verilerini MongoDB'ye kaydediyorum...\n",
      "GDELT verileri MongoDB'ye kaydedildi: 174635 kayıt\n",
      "\n",
      "Örnek GDELT kayıtları:\n",
      "Tarih: None\n",
      "Başlık: 27\n",
      "Metin: 28\n",
      "---\n",
      "Tarih: 2023-01-01\n",
      "Başlık: 60\n",
      "Metin: 6\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, lower, lit, date_format\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# MongoDB bağlantısı\n",
    "client = MongoClient('mongodb://mongodb:27017/')\n",
    "db = client['climatewatch']\n",
    "\n",
    "# Spark session başlat\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClimateWatch ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Veri yolları\n",
    "GDELT_PATH = \"/home/jovyan/work/data_storage/gdelt/*.csv\"\n",
    "CLIMATE_PATH = \"/home/jovyan/work/data_storage/climate/*.csv\"\n",
    "\n",
    "# GDELT verilerini oku ve dönüştür\n",
    "print(\"GDELT verilerini okuyorum...\")\n",
    "gdelt_df = spark.read.option(\"header\", False).csv(GDELT_PATH)\n",
    "\n",
    "# GDELT verilerini düzgün şekilde dönüştür\n",
    "gdelt_df = gdelt_df \\\n",
    "    .withColumn(\"date\", to_date(col(\"_c1\"), \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"date_str\", date_format(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"title\", col(\"_c27\")) \\\n",
    "    .withColumn(\"text\", col(\"_c28\")) \\\n",
    "    .withColumn(\"news_source_type\", lit(\"news\")) \\\n",
    "    .select(\"date_str\", \"title\", \"text\", \"news_source_type\")\n",
    "\n",
    "# GDELT verilerini MongoDB'ye kaydet\n",
    "print(\"GDELT verilerini MongoDB'ye kaydediyorum...\")\n",
    "gdelt_collection = db['gdelt_events']\n",
    "gdelt_collection.delete_many({})  # Mevcut verileri temizle\n",
    "\n",
    "# Pandas DataFrame'e dönüştür ve MongoDB'ye kaydet\n",
    "gdelt_pandas = gdelt_df.toPandas()\n",
    "gdelt_records = gdelt_pandas.to_dict('records')\n",
    "gdelt_collection.insert_many(gdelt_records)\n",
    "print(f\"GDELT verileri MongoDB'ye kaydedildi: {len(gdelt_records)} kayıt\")\n",
    "\n",
    "# Örnek verileri göster\n",
    "print(\"\\nÖrnek GDELT kayıtları:\")\n",
    "gdelt_sample = list(gdelt_collection.find().limit(2))\n",
    "for doc in gdelt_sample:\n",
    "    print(f\"Tarih: {doc.get('date_str')}\")\n",
    "    print(f\"Başlık: {doc.get('title')}\")\n",
    "    print(f\"Metin: {doc.get('text')}\")\n",
    "    print(\"---\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f366a41-6b7e-4bb0-aa03-73df89933b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Using cached pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.11/site-packages (4.13.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.11/site-packages (from pdfplumber) (10.1.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m766.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber) (3.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber) (41.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.24.3)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kaggle) (68.2.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.66.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: text-unidecode, python-slugify, pypdfium2, kaggle, pdfminer.six, pdfplumber\n",
      "Successfully installed kaggle-1.7.4.5 pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber pandas pyyaml pymongo requests beautifulsoup4 kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad76d131-c2b2-40bf-b2bb-2c57223d80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/jovyan/.config/kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4513fb-e7c6-40e2-995b-5a612970b1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/jovyan/.config/kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/jovyan/.config/kaggle/kaggle.json\n",
    "{\n",
    "    \"username\": \"denizardanarer\",\n",
    "    \"key\": \"bd9af2f2e1a558c5343945cd552787db\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4f1c6c-3211-464e-942b-deec35f334ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 /home/jovyan/.config/kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb088da1-ef46-4c57-8817-ff8bb2094417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "-rw------- 1 jovyan users 84 May 25 16:04 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/jovyan/.config/kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a6e5ec-4ad1-430b-9fa4-139b93f82067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240101 verisi indirildi ve kaydedildi: data_storage/gdelt/20240101.csv\n",
      "Filtrelenen satır sayısı: 0\n",
      "20240102 verisi indirildi ve kaydedildi: data_storage/gdelt/20240102.csv\n",
      "Filtrelenen satır sayısı: 0\n",
      "Veri indirildi ve kaydedildi: data_storage/climate/global_temp.csv\n",
      "Veri MongoDB'ye kaydedildi.\n",
      "Afet verisi kaydedildi: data_storage/disasters/disasters_20240101_20240102.json\n",
      "Politika değişikliği verisi kaydedildi: data_storage/policies/climate_policy_changes.csv\n",
      "Lütfen UNFCCC COP zirveleri CSV dosyasını indirip --csv_path ile belirtin.\n",
      "Kaggle'da 'climate' için veri kümeleri aranıyor...\n",
      "Veri seti indiriliyor: tarunrm09/climate-change-indicators\n",
      "Veri seti indirildi: tarunrm09/climate-change-indicators\n",
      "Veri seti indiriliyor: sumanthvrao/daily-climate-time-series-data\n",
      "Veri seti indirildi: sumanthvrao/daily-climate-time-series-data\n",
      "Veri seti indiriliyor: berkeleyearth/climate-change-earth-surface-temperature-data\n",
      "Veri seti indirildi: berkeleyearth/climate-change-earth-surface-temperature-data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Çalışma dizinini ayarla\n",
    "os.chdir('/home/jovyan/work')\n",
    "\n",
    "# Python yoluna proje dizinini ekle\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "# Gerekli modülleri import et\n",
    "from data_ingestion.run_ingestion import run_all_ingestions\n",
    "import yaml\n",
    "\n",
    "# Config dosyasını yükle\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Tüm veri kaynaklarını topla\n",
    "run_all_ingestions(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3343770c-c1c0-44d9-9f81-e72c585afe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB bağlantısı test ediliyor...\n",
      "{'version': '6.0.23', 'gitVersion': '4f70e0aaccb6426c94b440aaad5be5358d755c57', 'modules': [], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [6, 0, 23, 0], 'openssl': {'running': 'OpenSSL 3.0.2 15 Mar 2022', 'compiled': 'OpenSSL 3.0.2 15 Mar 2022'}, 'buildEnvironment': {'distmod': 'ubuntu2204', 'distarch': 'x86_64', 'cc': '/opt/mongodbtoolchain/v3/bin/gcc: gcc (GCC) 8.5.0', 'ccflags': '-Werror -include mongo/platform/basic.h -ffp-contract=off -fasynchronous-unwind-tables -ggdb -Wall -Wsign-compare -Wno-unknown-pragmas -Winvalid-pch -fno-omit-frame-pointer -fno-strict-aliasing -O2 -march=sandybridge -mtune=generic -mprefer-vector-width=128 -Wno-unused-local-typedefs -Wno-unused-function -Wno-deprecated-declarations -Wno-unused-const-variable -Wno-unused-but-set-variable -Wno-missing-braces -fstack-protector-strong -fdebug-types-section -Wa,--nocompress-debug-sections -fno-builtin-memcmp', 'cxx': '/opt/mongodbtoolchain/v3/bin/g++: g++ (GCC) 8.5.0', 'cxxflags': '-Woverloaded-virtual -Wno-maybe-uninitialized -fsized-deallocation -std=c++17', 'linkflags': '-Wl,--fatal-warnings -pthread -Wl,-z,now -fuse-ld=gold -fstack-protector-strong -fdebug-types-section -Wl,--no-threads -Wl,--build-id -Wl,--hash-style=gnu -Wl,-z,noexecstack -Wl,--warn-execstack -Wl,-z,relro -Wl,--compress-debug-sections=none -Wl,-z,origin -Wl,--enable-new-dtags', 'target_arch': 'x86_64', 'target_os': 'linux', 'cppdefines': 'SAFEINT_USE_INTRINSICS 0 PCRE_STATIC NDEBUG _XOPEN_SOURCE 700 _GNU_SOURCE _FORTIFY_SOURCE 2 BOOST_THREAD_VERSION 5 BOOST_THREAD_USES_DATETIME BOOST_SYSTEM_NO_DEPRECATED BOOST_MATH_NO_LONG_DOUBLE_MATH_FUNCTIONS BOOST_ENABLE_ASSERT_DEBUG_HANDLER BOOST_LOG_NO_SHORTHAND_NAMES BOOST_LOG_USE_NATIVE_SYSLOG BOOST_LOG_WITHOUT_THREAD_ATTR ABSL_FORCE_ALIGNED_ACCESS'}, 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'wiredTiger'], 'ok': 1.0}\n",
      "MongoDB bağlantısı başarılı!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://mongodb:27017/', serverSelectionTimeoutMS=5000)\n",
    "try:\n",
    "    print('MongoDB bağlantısı test ediliyor...')\n",
    "    print(client.server_info())\n",
    "    print(\"MongoDB bağlantısı başarılı!\")\n",
    "except Exception as e:\n",
    "    print(\"MongoDB bağlantı hatası:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befe26ee-20f5-489a-84ad-338961fdb5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  gdelt_date_str title text news_source_type   Source climate_year  \\\n",
      "0           None    27   28             news     None         None   \n",
      "1     2023-01-01    60    6             news     gcag         2023   \n",
      "2     2023-01-01    60    6             news  GISTEMP         2023   \n",
      "3     2023-01-01    50    5             news     gcag         2023   \n",
      "4     2023-01-01    50    5             news  GISTEMP         2023   \n",
      "\n",
      "  temperature_anomaly climate_source_type  date country policy_type  \\\n",
      "0                None                None  None    None        None   \n",
      "1              1.1003             climate  None    None        None   \n",
      "2              1.1692             climate  None    None        None   \n",
      "3              1.1003             climate  None    None        None   \n",
      "4              1.1692             climate  None    None        None   \n",
      "\n",
      "  description impact_level policy_source_type policy_date_str  year location  \\\n",
      "0        None         None               None            None  None     None   \n",
      "1        None         None               None            None  2023    Dubai   \n",
      "2        None         None               None            None  2023    Dubai   \n",
      "3        None         None               None            None  2023    Dubai   \n",
      "4        None         None               None            None  2023    Dubai   \n",
      "\n",
      "  number summit_source_type summit_date_str  \n",
      "0   None               None            None  \n",
      "1    28              summit      2023-01-01  \n",
      "2    28              summit      2023-01-01  \n",
      "3    28              summit      2023-01-01  \n",
      "4    28              summit      2023-01-01  \n",
      "Index(['gdelt_date_str', 'title', 'text', 'news_source_type', 'Source',\n",
      "       'climate_year', 'temperature_anomaly', 'climate_source_type', 'date',\n",
      "       'country', 'policy_type', 'description', 'impact_level',\n",
      "       'policy_source_type', 'policy_date_str', 'year', 'location', 'number',\n",
      "       'summit_source_type', 'summit_date_str'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Küçük/orta boyutlu ise:\n",
    "df = pd.read_parquet('data_storage/processed/combined_data.parquet')\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbefa230-5fc1-4611-93c3-bd0612d02b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB bağlantısı başarılı\n",
      "Spark bağlantısı başarılı\n",
      "GDELT verilerini okuyorum...\n",
      "GDELT veri sayısı: 174635\n",
      "\n",
      "İklim verilerini okuyorum...\n",
      "İklim veri sayısı: 319\n",
      "\n",
      "Kaggle verilerini okuyorum...\n",
      "climate_change_indicators.csv veri sayısı: 225\n",
      "DailyDelhiClimateTest.csv veri sayısı: 114\n",
      "DailyDelhiClimateTrain.csv veri sayısı: 1462\n",
      "GlobalLandTemperaturesByCity.csv veri sayısı: 8599212\n",
      "GlobalLandTemperaturesByCountry.csv veri sayısı: 577462\n",
      "GlobalLandTemperaturesByMajorCity.csv veri sayısı: 239177\n",
      "GlobalLandTemperaturesByState.csv veri sayısı: 645675\n",
      "GlobalTemperatures.csv veri sayısı: 3192\n",
      "\n",
      "Politika verilerini okuyorum...\n",
      "climate_policy_changes.csv veri sayısı: 2\n",
      "\n",
      "Açık veri setlerini okuyorum...\n",
      "\n",
      "COP zirveleri verilerini okuyorum...\n",
      "cop_summits.csv veri sayısı: 28\n",
      "\n",
      "Verileri dönüştürüyorum...\n",
      "\n",
      "Verileri birleştiriyorum...\n",
      "\n",
      "Sonuçları kaydediyorum...\n",
      "Birleşik veri data_storage/processed/combined_data.parquet olarak kaydedildi.\n",
      "\n",
      "MongoDB'ye kaydediyorum...\n",
      "GDELT verileri MongoDB'ye kaydedildi: 174635 kayıt\n",
      "İklim verileri MongoDB'ye kaydedildi: 319 kayıt\n",
      "climate_change_indicators.csv - 225 kayıt kaydedildi\n",
      "climate_change_indicators.csv MongoDB'ye kaydedildi: 225 kayıt\n",
      "DailyDelhiClimateTest.csv - 114 kayıt kaydedildi\n",
      "DailyDelhiClimateTest.csv MongoDB'ye kaydedildi: 114 kayıt\n",
      "DailyDelhiClimateTrain.csv - 1462 kayıt kaydedildi\n",
      "DailyDelhiClimateTrain.csv MongoDB'ye kaydedildi: 1462 kayıt\n",
      "GlobalLandTemperaturesByCity.csv dosyası MongoDB'ye aktarılmayacak (atlandı).\n",
      "GlobalLandTemperaturesByCountry.csv - 106744 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv - 105119 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv - 111574 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv - 110976 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv - 101916 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv - 41133 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByCountry.csv MongoDB'ye kaydedildi: 577462 kayıt\n",
      "GlobalLandTemperaturesByMajorCity.csv - 71074 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByMajorCity.csv - 71284 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByMajorCity.csv - 70136 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByMajorCity.csv - 26683 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByMajorCity.csv MongoDB'ye kaydedildi: 239177 kayıt\n",
      "GlobalLandTemperaturesByState.csv - 89471 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 86822 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 90694 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 86775 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 83684 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 89475 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 89485 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv - 29269 kayıt kaydedildi\n",
      "GlobalLandTemperaturesByState.csv MongoDB'ye kaydedildi: 645675 kayıt\n",
      "GlobalTemperatures.csv - 3192 kayıt kaydedildi\n",
      "GlobalTemperatures.csv MongoDB'ye kaydedildi: 3192 kayıt\n",
      "climate_policy_changes.csv MongoDB'ye kaydedildi: 2 kayıt\n",
      "cop_summits.csv MongoDB'ye kaydedildi: 28 kayıt\n",
      "Spark oturumu kapatıldı\n"
     ]
    }
   ],
   "source": [
    "from data_processing.spark_etl_pipeline import process_data\n",
    "\n",
    "# ETL işlemini başlat\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53dc0770-1b66-4f64-9331-cbe54952ecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+----+----------------+------+------------+-------------------+-------------------+----------+-------+------------------+--------------------+------------+------------------+---------------+----+--------+------+------------------+---------------+\n",
      "|gdelt_date_str|title|text|news_source_type|Source|climate_year|temperature_anomaly|climate_source_type|      date|country|       policy_type|         description|impact_level|policy_source_type|policy_date_str|year|location|number|summit_source_type|summit_date_str|\n",
      "+--------------+-----+----+----------------+------+------------+-------------------+-------------------+----------+-------+------------------+--------------------+------------+------------------+---------------+----+--------+------+------------------+---------------+\n",
      "|    2024-01-01|   51|   5|            news|  gcag|        2024|             1.1755|            climate|2024-01-01|    USA|Emission Reduction|New emission redu...|        High|            policy|     2024-01-01|NULL|    NULL|  NULL|              NULL|           NULL|\n",
      "|    2024-01-01|   43|   4|            news|  gcag|        2024|             1.1755|            climate|2024-01-01|    USA|Emission Reduction|New emission redu...|        High|            policy|     2024-01-01|NULL|    NULL|  NULL|              NULL|           NULL|\n",
      "|    2024-01-01|   25|   2|            news|  gcag|        2024|             1.1755|            climate|2024-01-01|    USA|Emission Reduction|New emission redu...|        High|            policy|     2024-01-01|NULL|    NULL|  NULL|              NULL|           NULL|\n",
      "|    2024-01-01|   17|   1|            news|  gcag|        2024|             1.1755|            climate|2024-01-01|    USA|Emission Reduction|New emission redu...|        High|            policy|     2024-01-01|NULL|    NULL|  NULL|              NULL|           NULL|\n",
      "|    2024-01-01|   42|   4|            news|  gcag|        2024|             1.1755|            climate|2024-01-01|    USA|Emission Reduction|New emission redu...|        High|            policy|     2024-01-01|NULL|    NULL|  NULL|              NULL|           NULL|\n",
      "+--------------+-----+----+----------------+------+------------+-------------------+-------------------+----------+-------+------------------+--------------------+------------+------------------+---------------+----+--------+------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- gdelt_date_str: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- news_source_type: string (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- climate_year: string (nullable = true)\n",
      " |-- temperature_anomaly: string (nullable = true)\n",
      " |-- climate_source_type: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- policy_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- impact_level: string (nullable = true)\n",
      " |-- policy_source_type: string (nullable = true)\n",
      " |-- policy_date_str: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- summit_source_type: string (nullable = true)\n",
      " |-- summit_date_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet('data_storage/processed/combined_data.parquet')\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd02488a-62ec-45d3-acc0-1e9987b35ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"temperature_anomaly\" is not a numeric column. Aggregation function can only be applied on a numeric column.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m news_counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_source_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Yıllara göre ortalama sıcaklık anomalisi\u001b[39;00m\n\u001b[1;32m     11\u001b[0m temp_anomaly \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature_anomaly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNotNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature_anomaly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2. Pandas'a çevir\u001b[39;00m\n\u001b[1;32m     15\u001b[0m news_counts_pd \u001b[38;5;241m=\u001b[39m news_counts\u001b[38;5;241m.\u001b[39mtoPandas()\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py:49\u001b[0m, in \u001b[0;36mdf_varargs_api.<locals>._api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_api\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupedData\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     48\u001b[0m     name \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m---> 49\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"temperature_anomaly\" is not a numeric column. Aggregation function can only be applied on a numeric column."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, to_date, col\n",
    "\n",
    "# 1. Spark ile özet çıkar\n",
    "df = df.withColumn('year', year(to_date(col('gdelt_date_str'))))\n",
    "\n",
    "# Yıllara göre haber sayısı\n",
    "news_counts = df.filter(df['news_source_type'] == 'news') \\\n",
    "                .groupBy('year').count().orderBy('year')\n",
    "\n",
    "# Yıllara göre ortalama sıcaklık anomalisi\n",
    "temp_anomaly = df.filter(df['temperature_anomaly'].isNotNull()) \\\n",
    "                 .groupBy('year').avg('temperature_anomaly').orderBy('year')\n",
    "\n",
    "# 2. Pandas'a çevir\n",
    "news_counts_pd = news_counts.toPandas().set_index('year')\n",
    "temp_anomaly_pd = temp_anomaly.toPandas().set_index('year')\n",
    "\n",
    "# Ortak yıllar\n",
    "common_years = news_counts_pd.index.intersection(temp_anomaly_pd.index)\n",
    "\n",
    "# 3. Görselleştir\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(common_years, news_counts_pd.loc[common_years, 'count'], label='Haber Sayısı', marker='o')\n",
    "plt.plot(common_years, temp_anomaly_pd.loc[common_years, 'avg(temperature_anomaly)'], label='Ortalama Sıcaklık Anomalisi', marker='o')\n",
    "plt.xlabel('Yıl')\n",
    "plt.ylabel('Değer')\n",
    "plt.title('Yıllara Göre Haber Sayısı ve Sıcaklık Anomalisi')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7b4d02-36d2-4c8d-9961-711d300e66b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id gdelt_date_str title text news_source_type\n",
      "0  6833500a6958af9257635d2f           None    27   28             news\n",
      "1  6833500a6958af9257635d30     2023-01-01    60    6             news\n",
      "2  6833500a6958af9257635d31     2023-01-01    50    5             news\n",
      "3  6833500a6958af9257635d32     2023-01-01    51    5             news\n",
      "4  6833500a6958af9257635d33     2023-01-01    51    5             news\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient('mongodb://mongodb:27017/')\n",
    "db = client['climatewatch']\n",
    "\n",
    "# Örnek: GDELT verisi\n",
    "gdelt_df = pd.DataFrame(list(db['gdelt_events'].find()))\n",
    "print(gdelt_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1400806-08ec-4862-9afb-0511cc81313c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_CONVERT_COLUMN_INTO_BOOL] Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Parquet veya MongoDB'den okuduğunuz DataFrame'de tarih sütunu varsa:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgdelt_date_str\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoerce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_index())\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_index()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1045\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCSeries\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1046\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/dtypes/generic.py:44\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/dtypes/generic.py:38\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._check\u001b[0;34m(inst)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check\u001b[39m(inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_typ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomp\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:1400\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1401\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_CONVERT_COLUMN_INTO_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1402\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1403\u001b[0m     )\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_CONVERT_COLUMN_INTO_BOOL] Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "# Parquet veya MongoDB'den okuduğunuz DataFrame'de tarih sütunu varsa:\n",
    "df['year'] = pd.to_datetime(df['gdelt_date_str'], errors='coerce').dt.year\n",
    "print(df['year'].value_counts().sort_index())\n",
    "df['year'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8fb35de-0414-49cd-8918-b0df8895dc38",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'rename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Yıllık haber sayısı ve sıcaklık ortalaması\u001b[39;00m\n\u001b[1;32m      2\u001b[0m yearly \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature_anomaly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_count\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(yearly)\n\u001b[1;32m      7\u001b[0m yearly\u001b[38;5;241m.\u001b[39mplot(y\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature_anomaly\u001b[39m\u001b[38;5;124m'\u001b[39m], secondary_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature_anomaly\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3125\u001b[0m     )\n\u001b[1;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'rename'"
     ]
    }
   ],
   "source": [
    "# Yıllık haber sayısı ve sıcaklık ortalaması\n",
    "yearly = df.groupby('year').agg({\n",
    "    'title': 'count',\n",
    "    'temperature_anomaly': 'mean'\n",
    "}).rename(columns={'title': 'news_count'})\n",
    "print(yearly)\n",
    "yearly.plot(y=['news_count', 'temperature_anomaly'], secondary_y='temperature_anomaly', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eb095-6ed2-49c5-997e-7efe43064af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
